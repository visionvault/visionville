<!DOCTYPE html>
<html>

<head>
    <title>Research & News Page</title>
    <!-- Bootstrap CSS -->
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css">
</head>

<body>
    <div class="container mt-5">
        <h1 class="text-center">Research & News Page</h1>

        <h2>Search</h2>
        <div class="input-group mb-3">
            <input type="text" id="search-input" class="form-control" placeholder="Search for information...">
        </div>

        <h2>Research Articles</h2>
        <div id="research-container">
            <div class="card">
                <div class="card-body">
                    <h5 class="card-title">FPO++: Efficient Encoding and Rendering of Dynamic Neural Radiance Fields by Analyzing and Enhancing Fourier PlenOctrees</h5>
                    <p class="card-text">Fourier PlenOctrees have shown to be an efficient representation for\nreal-time rendering of dynamic Neural Radiance Fields (NeRF). Despite its many\nadvantages, this method suffers from artifacts introduced by the involved\ncompression when combining it with recent state-of-the-art techniques for\ntraining the static per-frame NeRF models. In this paper, we perform an\nin-depth analysis of these artifacts and leverage the resulting insights to\npropose an improved representation. In particular, we present a novel density\nencoding that adapts the Fourier-based compression to the characteristics of\nthe transfer function used by the underlying volume rendering procedure and\nleads to a substantial reduction of artifacts in the dynamic model.\nFurthermore, we show an augmentation of the training data that relaxes the\nperiodicity assumption of the compression. We demonstrate the effectiveness of\nour enhanced Fourier PlenOctrees in the scope of quantitative and qualitative\nevaluations on synthetic and real-world scenes.</p>
                    <a href="http://arxiv.org/abs/2310.20710v1" class="btn btn-primary" target="_blank">Read more</a>
                </div>
            </div>
            <div class="card">
                <div class="card-body">
                    <h5 class="card-title">What's In My Big Data?</h5>
                    <p class="card-text">Large text corpora are the backbone of language models. However, we have a\nlimited understanding of the content of these corpora, including general\nstatistics, quality, social factors, and inclusion of evaluation data\n(contamination). In this work, we propose What's In My Big Data? (WIMBD), a\nplatform and a set of sixteen analyses that allow us to reveal and compare the\ncontents of large text corpora. WIMBD builds on two basic capabilities -- count\nand search -- at scale, which allows us to analyze more than 35 terabytes on a\nstandard compute node. We apply WIMBD to ten different corpora used to train\npopular language models, including C4, The Pile, and RedPajama. Our analysis\nuncovers several surprising and previously undocumented findings about these\ncorpora, including the high prevalence of duplicate, synthetic, and low-quality\ncontent, personally identifiable information, toxic language, and benchmark\ncontamination. For instance, we find that about 50% of the documents in\nRedPajama and LAION-2B-en are duplicates. In addition, several datasets used\nfor benchmarking models trained on such corpora are contaminated with respect\nto important benchmarks, including the Winograd Schema Challenge and parts of\nGLUE and SuperGLUE. We open-source WIMBD's code and artifacts to provide a\nstandard set of evaluations for new text-based corpora and to encourage more\nanalyses and transparency around them: github.com/allenai/wimbd.</p>
                    <a href="http://arxiv.org/abs/2310.20707v1" class="btn btn-primary" target="_blank">Read more</a>
                </div>
            </div>
            <div class="card">
                <div class="card-body">
                    <h5 class="card-title">DDAM-PS: Diligent Domain Adaptive Mixer for Person Search</h